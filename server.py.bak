import os
from flask import Flask, request, jsonify
from transformers import AutoTokenizer
from ctransformers import AutoModelForCausalLM
from optimum.onnxruntime import ORTModelForSeq2SeqLM
import torch

# Create a Flask object
app = Flask("Dolphin-2.6.mistral-7b server")

# Ensure that the model uses GPU if available 
device = "cuda" if torch.cuda.is_available() else "cpu"
print(f"'device' set to '{device}'")

# Get the model path from environment variable or use default
#model_path = "TheBloke/dolphin-2.6-mistral-7B-dpo-laser-GGUF" #change for env variable
model_name = "gpt2"
tokenizer = AutoTokenizer.from_pretrained(model_name)
model = AutoModelForCausalLM.from_pretrained(model_name).to(device)
# Template for the prompt
template = "system\n{system_context}\nuser\n{user_prompt}\nassistant\n{assistant_context}"

@app.route('/dolphin', methods=['POST'])
def generate_response():
    global model
    global tokenizer
    global device
    
    try:
        data = request.get_json()

        # Check if the required fields are present in the JSON data
        if 'system_context' in data and 'user_prompt' in data and 'max_tokens' in data and 'assistant_context' in data:
            system_context = data['system_context']
            user_prompt = data['user_prompt']
            max_tokens = int(data['max_tokens'])
            assistant_context = data['assistant_context']

            # Create the prompt using the template
            prompt = template.format(system_context=system_context, user_prompt=user_prompt, assistant_context=assistant_context)
            
            # Generate response using the model
            print(f"Tokenizing") # debug
            inputs = tokenizer(prompt, return_tensors="pt").to(device)
            print(f"Generating output") # debug
            # Generate text
            with torch.no_grad():
                outputs = model.generate(inputs['input_ids'], max_length=100)
            print(f"Tokenizer decoding") # debug
            generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)
            print(f"Sending response") # debug
            return jsonify({'generated_text': generated_text})

        else:
            return jsonify({"error": "Missing required parameters"}), 400

    except Exception as e:
        return jsonify({"Error": str(e)}), 500

if __name__ == '__main__':
    print("Starting server")
    app.run(host='0.0.0.0', port=8000, debug=True)
